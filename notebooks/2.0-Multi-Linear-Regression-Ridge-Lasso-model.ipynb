{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "38fb3ec7",
   "metadata": {},
   "source": [
    "# Multi Linear Regression Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0a58acc3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Libraries for data loading, data manipulation and data visulisation\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from scipy import stats\n",
    "import boto3\n",
    "sns.set()\n",
    "\n",
    "# Libraries for data preparation and model building\n",
    "\n",
    "import statsmodels.api as sm\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.linear_model import RidgeCV\n",
    "from sklearn.linear_model import LassoCV\n",
    "from sklearn.neighbors import KNeighborsRegressor\n",
    "from sklearn.tree import DecisionTreeRegressor\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn.model_selection import RepeatedKFold\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from mlxtend.feature_selection import SequentialFeatureSelector as sfs\n",
    "import pickle\n",
    "\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from sklearn.metrics import r2_score\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "import sys\n",
    "# Insert the parent path relative to this notebook so we can import from the src folder.\n",
    "sys.path.insert(0, \"..\")\n",
    "from src.data.make_dataset import split_data\n",
    "from src.data.make_dataset import reg_metrics\n",
    "\n",
    "\n",
    "# Import modules for VIF\n",
    "from statsmodels.stats.outliers_influence import variance_inflation_factor\n",
    "\n",
    "import warnings\n",
    "\n",
    "# Surpress warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "pd.set_option('display.max_columns', None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b8fb3614",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Instantiate boto3 by providing access and secrete keys\n",
    "client = boto3.client('s3', aws_access_key_id='AKIATNJHRXAPUA4DIFER', aws_secret_access_key=\"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\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9288a0bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# S3 bucket name\n",
    "bucket = \"2207-17-fibre-competitive-intensity-model-b\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "028d96d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate a file path to the S3 bucket\n",
    "uptake_file_path = 'https://2207-17-fibre-competitive-intensity-model-b.s3.eu-west-1.amazonaws.com/Data+for+Modeling/municipality-data-for-modelling.csv'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0b858491",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the dataset\n",
    "data = pd.read_csv(uptake_file_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e78ef31f",
   "metadata": {},
   "source": [
    "## Exploratory Data Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "35251f5c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Preview the top 5 rows of the dataset\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b8a8f01e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check for correlation between target variable and the features\n",
    "data.corr(numeric_only=True)['uptake_rate_hh'].sort_values(ascending=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6638d38a",
   "metadata": {},
   "source": [
    "Quite a number of the features have strong correlation with the target variable. This is a strong indication that a linear model may perform well with this dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "38702ee0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check for multicolinearity\n",
    "plt.figure(figsize=(25,15))\n",
    "sns.heatmap(data.corr(numeric_only=True),\n",
    "            vmin = -1, \n",
    "            vmax = 1,\n",
    "            fmt=\".1f\",\n",
    "            cmap =\"GnBu\",\n",
    "            annot=True)\n",
    "plt.xticks(fontsize=15)\n",
    "plt.yticks(fontsize=15)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "edcd4ee8",
   "metadata": {},
   "source": [
    "Multi colinearity can be observed among different features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f88271b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check for missing values\n",
    "data.isnull().sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1d177b79",
   "metadata": {},
   "source": [
    "There is a municipality missing some demographic data. Since it is just a single municipality and the demographic infrmation missing are crucial for the model development. This municipality should be dropped in order not to affect negative effect on the model to be developed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6410f3c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check the distribution of the target variable\n",
    "sns.histplot(data['uptake_rate_hh'], kde=True);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cccf296e",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"skewness:\", data['uptake_rate_hh'].skew())\n",
    "print(\"Kurtosis:\", data['uptake_rate_hh'].kurtosis())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a43ffade",
   "metadata": {},
   "source": [
    "The target variable is a right-skewed distribution  with a skewness of 2.3 and kutosis of 6, giving us a leptokurtic distribution. All these affirms that the target variable is not normally distributed, and It would not be wise to develop a linear model with such data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cf864e91",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check the data types of the dataset\n",
    "data.dtypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f5ffaa5e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check why the \"Per annum\" column is of object dtype\n",
    "data['Population growth Per annum']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8a50a128",
   "metadata": {},
   "source": [
    "This feature still contains unclean data. It will be dropped for now and preprocess properly if needed later."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c2b19a8d",
   "metadata": {},
   "source": [
    "#### Data Quality Issues\n",
    "\n",
    "- Missing data for several features for only one row\n",
    "- Multi colinearity issues\n",
    "- 'Per anum\" column contains a special character\n",
    "- Target variable is right skewed\n",
    "- The lower and higher income class were combined to get the average income.\n",
    "- The 'average househol size' and the 'DERH_HSIZE' contains thesame information about the average household size. The former is the most recent"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b2d65db8",
   "metadata": {},
   "source": [
    "## Data Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ed916bff",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Drop rows with missing values\n",
    "data = data.dropna(axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c0dc6aea",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check to be sure missing rows were dropped\n",
    "data.isnull().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ca3730d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert emploment status as a % of the population\n",
    "data['DERH_HH_EMPLOY_STATUS'] = data['DERH_HH_EMPLOY_STATUS']/data['population'] *100\n",
    "data['H_GEOTYPE'] = data['H_GEOTYPE']/data['households'] *100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b431e98d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Drop columns that are not required for model development\n",
    "df = data.drop(['Unnamed: 0.1','Unnamed: 0','Population growth Per annum','avg_d_kbps','avg_u_kbps','avg_lat_ms','fiber','devices','CAT2',\n",
    "                'DISTRICT_N','total_tiles','uptake_rate/population','uptake_rate/households','H_MUNIC','population',\n",
    "                'DERH_HSIZE','Income_Class_Lower','Income_Class_Higher','DERH_HINCOME','households'], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e0b8f31a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract data points with uptake rates only\n",
    "df = df[df['uptake_rate_hh'] != 0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c5091e39",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set municipality as index\n",
    "df = df.set_index('municipality')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b86c376a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Normalize the target variable\n",
    "sns.histplot(np.log(df['uptake_rate_hh']), kde=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d9e46fba",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Normalize the target variable\n",
    "df['uptake_rate_hh_norm'] = np.log(df['uptake_rate_hh'])\n",
    "df['uptake_rate_pop_norm'] = np.log(df['uptake_rate_pop'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d52f03e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot the distribution of the fiber speed test\n",
    "fig, axes = plt.subplots(1, 2, figsize =(15,3))\n",
    "\n",
    "sns.histplot(df['uptake_rate_pop'], kde=True, ax = axes[0])\n",
    "axes[0].set_title(\"Uptake_rate\")\n",
    "\n",
    "sns.histplot(df['uptake_rate_pop_norm'], kde=True, ax = axes[1])\n",
    "axes[1].set_title(\"Normalized_uptake_rate\")\n",
    "\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b33e5f50",
   "metadata": {},
   "source": [
    "## Check for OLS Assumptions\n",
    "\n",
    "### OLS Assumptions include:\n",
    "\n",
    "- Linearity\n",
    "- No Endogeneity\n",
    "- Normality and Homoscedasticity\n",
    "- No Autocorrelation\n",
    "- No Multicolinearity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4bd1453a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# def check_linearity(df):\n",
    "#     '''\n",
    "#     This function takes in a dataframe and return a scatter plot visual of the first three columns\n",
    "#     of the dataframe's features against the dependent variable.\n",
    "    \n",
    "#     '''\n",
    "#     columns = df.columns\n",
    "    \n",
    "#     f, (ax1, ax2, ax3) = plt.subplots(1, 3, sharey=True, figsize =(15,3)) #sharey -> share 'Price' as y\n",
    "#     ax1.scatter(df[columns[0]], df['uptake_rate_hh'])\n",
    "#     ax1.set_title(f'load_shortfall_3h and {columns[0]}')\n",
    "# #     ax2.scatter(df[columns[1]], df['uptake_rate_hh'])\n",
    "# #     ax2.set_title(f'load_shortfall_3h and {columns[1]}')\n",
    "# #     ax3.scatter(df[columns[2]], df['uptake_rate_hh'])\n",
    "# #     ax3.set_title(f'load_shortfall_3h and {columns[2]}')\n",
    "    \n",
    "#     return plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1059f4df",
   "metadata": {},
   "outputs": [],
   "source": [
    "features = df.columns\n",
    "\n",
    "for i in features:\n",
    "    sns.histplot(df[i], kde=True)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5bf2124b",
   "metadata": {},
   "source": [
    "### No Endogeneity of the Regressors Assumption\n",
    "\n",
    "- Endgeneity occurs when there is Ommited Variable Bias, therfore making our independendent variables to be correlated with the Residuals. Relaxing this assumption can be very difficult.\n",
    ">\n",
    "- We could explore more on this assumption if Multilinear regression is fit for our model."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "780d06e5",
   "metadata": {},
   "source": [
    "### Normality and Homoscedasticity Assumption\n",
    "\n",
    "- Normality is assumed for a big sample using central limit theorem\n",
    "- Zero mean of the distribution of errors is accomplished with the inclusion of intercept in the regression\n",
    "- Homoscedasticity means equal varience among the error terms.\n",
    "\n",
    "In this case, we know that the target variable is not normally distributed. But the Normalization done earlier is suficient for now."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e94a93a2",
   "metadata": {},
   "source": [
    "### No Autocorrelation Assumption\n",
    "\n",
    "- Autocorrelation assumes that errors are uncorrelated\n",
    "- This is hard to observe in a data that is taken one moment of a time.\n",
    "- it is very common with time series data\n",
    "- Since our data is not a time series data, we do expect to encounter autocorrelation problems. However, there is a need to verify this\n",
    "\n",
    "One way to check for autocorrelation is through the Durbin-Watson test, and furtunately, we can easily get this result using Statsmodels, on the summary stats of our trained model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4568e9e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use statsmodel to fit the data so as to print the summary of the model stats\n",
    "# Declare the target\n",
    "y = df['uptake_rate_hh_norm']\n",
    "\n",
    "# Declare the features\n",
    "x1 = df.drop(['uptake_rate_pop','uptake_rate_hh','uptake_rate_hh_norm','uptake_rate_pop_norm'], axis = 1)\n",
    "\n",
    "x2 = x1[['Average_Income','percent_higher_education','percent_piped_water_inside_dwelling','percent_no_schooling']]\n",
    "\n",
    "\n",
    "scalar_OLS = StandardScaler()\n",
    "x2_scale = scalar_OLS.fit_transform(x2)\n",
    "x2 = pd.DataFrame(x2_scale, columns=x2.columns, index = x2.index)\n",
    "x = sm.add_constant(x2)\n",
    "result = sm.OLS(y, x).fit()\n",
    "print(result.summary())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "610fcbcd",
   "metadata": {},
   "source": [
    "The Durbin-Watson score is approximately 2, this shows that there is no Autocorrelation in the dataset. We expected this since the dataset is a cross-sectional data. Also, there is no information on multicolinearity"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7416d3ce",
   "metadata": {},
   "source": [
    "### No Multicollinearity Assumption\n",
    "\n",
    "- This can be verified by computing the VIF"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4db53929",
   "metadata": {},
   "source": [
    "#### Variance Inflation Factor\n",
    "\n",
    "sklearn does not have a built-in way to check for multicollinearity\n",
    "one of the main reasons is that this is an issue well covered in statistical frameworks and not in ML ones\n",
    "So, to calculate VIF, we have to rely on statsmodels\n",
    "To make this as easy as possible to use, we declare a variable where we put\n",
    "all features where we want to check for multicollinearity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d85cf54c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# since all our data are numerical, we simply calculate our VIF\n",
    "variables = x1\n",
    "\n",
    "# we create a new data frame which will include all the VIFs\n",
    "# note that each variable has its own variance inflation factor as this measure is variable specific (not model specific)\n",
    "vif = pd.DataFrame()\n",
    "\n",
    "# here we make use of the variance_inflation_factor, which will basically output the respective VIFs \n",
    "vif[\"VIF\"] = [variance_inflation_factor(x1.values, i) for i in range(variables.shape[1])]\n",
    "# Finally, I like to include names so it is easier to explore the result\n",
    "vif[\"Features\"] = variables.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f62c3b85",
   "metadata": {},
   "outputs": [],
   "source": [
    "vif"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e2a26e85",
   "metadata": {},
   "source": [
    "From the VIF, all features are multi collinear if the team is use the rule of thumb threshold of 10. This is a clear indication multicolinearity and unfortunately, this can not be relax as that would mean removing almost all the features."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3c151d0a",
   "metadata": {},
   "source": [
    "## Declare the Inputs and Target Variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e486bd32",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Declare the targets and the inputs\n",
    "# The dependent variable is uptake_rate_hh_norm for parametric models\n",
    "target = df['uptake_rate_hh_norm']\n",
    "\n",
    "# The dependent variable is uptake_rate_hh for non parametric models\n",
    "target_nonp = df['uptake_rate_hh']\n",
    "\n",
    "# The inputs is everything BUT the dependent variables, so we can simply drop it\n",
    "inputs = df.drop(['uptake_rate_pop','uptake_rate_pop_norm','uptake_rate_hh','uptake_rate_hh_norm'], axis = 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bb61d713",
   "metadata": {},
   "source": [
    "## Separate Test dataset from the Train dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "94daa03a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Separate the testing data from the training data. We will use a 20% split here\n",
    "split_value = int(len(df) * 0.2)\n",
    "\n",
    "# Split the input and target data into teat and train components\n",
    "input_test = inputs.iloc[-split_value:, :]\n",
    "target_test = target.iloc[-split_value:]\n",
    "target_test_nonp = target_nonp.iloc[-split_value:]\n",
    "\n",
    "input_train = inputs.iloc[:-split_value, :]\n",
    "target_train = target.iloc[:-split_value]\n",
    "target_train_nonp = target_nonp.iloc[:-split_value]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8f220934",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split the data into training and testing set\n",
    "# For parametric models\n",
    "x_train, x_test, y_train, y_test = split_data(inputs, target, 0.2, 365)\n",
    "x_train, x_test, y_train_nonp, y_test_nonp = split_data(inputs, target_nonp, 0.2, 365)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "683eb985",
   "metadata": {},
   "source": [
    "## Scale the Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a01e9981",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Regularize the data by feature scaling\n",
    "# Create a scaler object\n",
    "scaler = StandardScaler()\n",
    "\n",
    "x_train_scaled = scaler.fit_transform(x_train)\n",
    "x_test_scaled = scaler.transform(x_test)\n",
    "\n",
    "x_train_scaled = pd.DataFrame(x_train_scaled, columns=x_train.columns, index = x_train.index)\n",
    "x_test_scaled = pd.DataFrame(x_test_scaled, columns=x_test.columns, index = x_test.index)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dc7d3435",
   "metadata": {},
   "source": [
    "## Model Development"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d7f35c6f",
   "metadata": {},
   "source": [
    "### Multi Linear Regression Algorithm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ce20273b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a linear regression object\n",
    "lm = LinearRegression()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "766e320d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make a copy of the train and test datasets\n",
    "x_train_copy = x_train_scaled.copy()\n",
    "x_test_copy = x_test_scaled.copy()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f25295eb",
   "metadata": {},
   "source": [
    "#### Step Forward Feature Selection\n",
    "\n",
    "According to [Kdnuggets](https://www.kdnuggets.com/2018/06/step-forward-feature-selection-python.html) Step forward feature selection starts with the evaluation of each individual feature, and selects that which results in the best performing selected algorithm model. What's the \"best?\" That depends entirely on the defined evaluation criteria (AUC, prediction accuracy, RMSE, etc.). Next, all possible combinations of the that selected feature and a subsequent feature are evaluated, and a second feature is selected, and so on, until the required predefined number of features is selected.\n",
    "\n",
    "Step backward feature selection is closely related, and as you may have guessed starts with the entire set of features and works backward from there, removing features to find the optimal subset of a predefined size."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8b38d418",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a feature selection object\n",
    "sfs1 = sfs(lm, k_features=4, forward=True, verbose=2, scoring='r2')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "461065e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the features with high predictive power\n",
    "sfs1 = sfs1.fit(x_train_copy,y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "556ac584",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Store the features in a list\n",
    "lr_features = list(sfs1.k_feature_names_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "683668e0",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "lr_features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "28aab4f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Slice training data with the features\n",
    "x_train_scaled = x_train_copy[lr_features]\n",
    "x_test_scaled = x_test_copy[lr_features]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "727e6e1a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fit the regression with the inputs and targets\n",
    "lm.fit(x_train_scaled,y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ad1133f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's check the outputs of the regression\n",
    "# I'll store them in y_hat as this is the 'theoretical' name of the predictions\n",
    "y_hat = lm.predict(x_test_scaled)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f0b94c9d",
   "metadata": {},
   "source": [
    "#### Evaluate Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7b2499e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute the metrics\n",
    "reg_metrics(y_test, y_hat,input_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ba9bb505",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the weights of each coefficients\n",
    "relevant_features = pd.DataFrame(lm.coef_, index=x_train_scaled.columns, \n",
    "                                 columns=['Coefficients']).sort_values(by='Coefficients',ascending=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a8593ce7",
   "metadata": {},
   "outputs": [],
   "source": [
    "relevant_features"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "34807643",
   "metadata": {},
   "source": [
    "#### Save The Best Performing Model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ef11bf7e",
   "metadata": {},
   "source": [
    "### Ridge Algorithm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "678ddb29",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize a repeated K-fold Cross Validator\n",
    "cv = RepeatedKFold(n_splits=10, n_repeats=5, random_state=365)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6c04c157",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Perform Ridge regression with repeated K-fold cross validator\n",
    "ridgecv = RidgeCV(alphas=np.arange(0.1, 15, 0.1), cv=cv, scoring='neg_mean_absolute_error')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "731c0051",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a feature selection object\n",
    "sfs1 = sfs(ridgecv, k_features=4, forward=True, verbose=2, scoring='r2')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dc6f3548",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the features with high predictive power\n",
    "sfs1 = sfs1.fit(x_train_copy,y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "562dd672",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Store the features in a list\n",
    "ridge_features = list(sfs1.k_feature_names_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "879068cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Slice training data with the features\n",
    "x_train_scaled = x_train_copy[ridge_features]\n",
    "x_test_scaled = x_test_copy[ridge_features]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "819034cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fitting the RidgeCV regressor\n",
    "ridgecv.fit(x_train_scaled, y_train)\n",
    "print(\"Ridge tuning parameter:\", (ridgecv.alpha_))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2be18cd6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Predict the y_test values\n",
    "y_hat = ridgecv.predict(x_test_scaled)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5b9743e2",
   "metadata": {},
   "source": [
    "#### Evaluate Algorithm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6529566a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute the metrics\n",
    "reg_metrics(y_test, y_hat,input_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "62ac41ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the weights of each coefficients\n",
    "relevant_features = pd.DataFrame(ridgecv.coef_, index=inputs.columns, \n",
    "                                 columns=['Coefficients']).sort_values(by='Coefficients',ascending=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2dc22af2",
   "metadata": {},
   "outputs": [],
   "source": [
    "relevant_features"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "18f2a815",
   "metadata": {},
   "source": [
    "### Lasso Algorithm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "81556495",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Perform Lasso regression with repeated K-fold cross validation\n",
    "lassocv = LassoCV(alphas=np.arange(0.1, 15, 0.1), cv=cv, tol = 0.3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "532c1c1f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a feature selection object\n",
    "sfs1 = sfs(lassocv, k_features=4, forward=True, verbose=2, scoring='r2')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a2e1b8b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the features with high predictive power\n",
    "sfs1 = sfs1.fit(x_train_copy,y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "02bf514b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Store the features in a list\n",
    "lasso_features = list(sfs1.k_feature_names_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "602b63e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Slice training data with the features\n",
    "x_train_scaled = x_train_copy[lasso_features]\n",
    "x_test_scaled = x_test_copy[lasso_features]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aa214f5e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fitting the Lassocv regressor\n",
    "lassocv.fit(x_train_scaled, y_train)\n",
    "print(\"Lasso tuning parameter:\", (lassocv.alpha_))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5bcf9c80",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Predict the y_test values\n",
    "y_hat = lassocv.predict(x_test_scaled)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "98ba8bc4",
   "metadata": {},
   "source": [
    "#### Evaluate the Algorithm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "79245c5e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute the metrics\n",
    "reg_metrics(y_test, y_hat,input_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f140e02b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the weights of each coefficients\n",
    "relevant_features = pd.DataFrame(lassocv.coef_, index=x_train_scaled.columns, \n",
    "                                 columns=['Coefficients']).sort_values(by='Coefficients',ascending=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7fb9cc63",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display the coefficients of the features\n",
    "relevant_features"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7adb3488",
   "metadata": {},
   "source": [
    "## Non-Parametric Models"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "747aca54",
   "metadata": {},
   "source": [
    "### KNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d3ff4d40",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a KNN regressor object\n",
    "knn = KNeighborsRegressor()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c9e4d135",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a feature selection object\n",
    "sfs1 = sfs(knn, k_features=4, forward=True, verbose=2, scoring='r2')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "93255544",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the features with high predictive power\n",
    "sfs1 = sfs1.fit(x_train_copy,y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4c68206b",
   "metadata": {},
   "outputs": [],
   "source": [
    "sfs1.k_feature_names_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d33e0a8c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Store the features in a list\n",
    "knn_features = list(sfs1.k_feature_names_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9a1d94e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Slice training data with the features\n",
    "x_train_scaled = x_train_copy[knn_features]\n",
    "x_test_scaled = x_test_copy[knn_features]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c93bcecd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train the model\n",
    "knn.fit(x_train_scaled, y_train_nonp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2d4b5aa3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Predict the y_test values\n",
    "y_hat = knn.predict(x_test_scaled)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "711d8ee6",
   "metadata": {},
   "source": [
    "#### Evaluate Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ca492bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute the metrics\n",
    "reg_metrics(y_test_nonp, y_hat,input_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b7bfa9a6",
   "metadata": {},
   "source": [
    "### Decission Tree Algorithm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3b343709",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a DecisionTree object\n",
    "reg_tree = DecisionTreeRegressor(random_state=365)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a5274d32",
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train_copy = x_train.copy()\n",
    "x_test_copy = x_test.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7bdc333b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a feature selection object\n",
    "sfs1 = sfs(reg_tree, k_features=4, forward=True, verbose=2, scoring='r2')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2a84a03b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the features with high predictive power\n",
    "sfs1 = sfs1.fit(x_train_copy,y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "17793e52",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Store the features in a list\n",
    "dt_features = list(sfs1.k_feature_names_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "84a032e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Slice training data with the features\n",
    "x_train = x_train_copy[dt_features]\n",
    "x_test = x_test_copy[dt_features]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "88371a31",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fit the model\n",
    "reg_tree.fit(x_train, y_train_nonp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "710270a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Predict the y_test values\n",
    "y_hat = reg_tree.predict(x_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4b0856db",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute the metrics\n",
    "reg_metrics(y_test_nonp, y_hat,input_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4559c954",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the weights of each coefficients\n",
    "DT_feature_importance = pd.DataFrame(reg_tree.feature_importances_, index=x_train.columns, \n",
    "                                 columns=['Importance']).sort_values(by='Importance',ascending=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "836f952a",
   "metadata": {},
   "outputs": [],
   "source": [
    "DT_feature_importance"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6edd696d",
   "metadata": {},
   "source": [
    "### Random Forest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "74de9d41",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a RandomForest object\n",
    "RF = RandomForestRegressor(n_estimators=200, max_features='sqrt',random_state=365)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d7f196ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a feature selection object\n",
    "sfs1 = sfs(RF, k_features=4, forward=True, verbose=2, scoring='r2')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "80a7c01d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the features with high predictive power\n",
    "sfs1 = sfs1.fit(x_train_copy,y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6bf255f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "sfs1.k_feature_names_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "77d58851",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Store the features in a list\n",
    "rf_features = list(sfs1.k_feature_names_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f9b44e53",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Slice training data with the features\n",
    "x_train = x_train_copy[rf_features]\n",
    "x_test = x_test_copy[rf_features]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cb77a059",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train the model\n",
    "RF.fit(x_train, y_train_nonp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "548bc2cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Predict the y_test values\n",
    "y_hat = RF.predict(x_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dca7d52b",
   "metadata": {},
   "source": [
    "#### Evaluate the Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3cf96f60",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute the metrics\n",
    "reg_metrics(y_test_nonp, y_hat,input_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6db3c98a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the weights of each coefficients\n",
    "RF_feature_importance = pd.DataFrame(RF.feature_importances_, index=x_train.columns, \n",
    "                                 columns=['Importance']).sort_values(by='Importance',ascending=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aa83c284",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Get the feature importance\n",
    "RF_feature_importance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c58c8343",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the RF model\n",
    "model_save_path = \"RF_municipal_model.pkl\"\n",
    "\n",
    "with open(model_save_path,'wb') as file:\n",
    "    pickle.dump(RF,file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c10d9e5f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the feature names of the model\n",
    "model_save_path = \"RF_municipal_features.pkl\"\n",
    "\n",
    "with open(model_save_path,'wb') as file:\n",
    "    pickle.dump(rf_features,file)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
